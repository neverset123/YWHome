---
layout:     post
title:      dask
subtitle:   
date:       2020-11-07
author:     neverset
header-img: img/post-bg-kuaidi.jpg
catalog: true
tags:
    - data engineering
---

Dask is a parallel computing library that works by distributing larger computations and breaking it down into smaller computations through a task scheduler and task workers.
it consists of three components:
* scheduler
* workers
* One or multiple clients

## structure

![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20201108000715.png)

* High-level collections:
Dask provides high-level Array, Bag, and DataFrame collections that mimic NumPy, lists, and Pandas but can operate in parallel on datasets that don’t fit into main memory. Dask’s high-level collections are alternatives to NumPy and Pandas for large datasets.
* Low-Level schedulers: 
Dask provides dynamic task schedulers that execute task graphs in parallel. Used as an alternative to direct use of threading or multiprocessing libraries in complex cases or other task scheduling systems like Luigi or IPython parallel.

### Dask.array
multidimensional array composed of many small NumPy arrays
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20201108001345.png)

    import dask.array as da
    x = da.random.random((10000, 10000), chunks=(1000, 1000))

### Dask.DataFrame
Dask DataFrame is a logical connection of many Pandas DataFrames
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20201108001554.png)

    from dask import datasets
    import dask.dataframe as dd
    df = datasets.timeseries()

### Visualizing task graphs
#### save local image
    import dask.array as da
    x = da.ones((50, 50), chunks=(5, 5))
    y = x + x.T
    # y.compute()
    y.visualize(filename='transpose.svg')
#### dask dashboard

## Install und usage

    $ pip install dask[complete]
    $ conda install dask
    from dask.distributed import LocalCluster, Client
    cluster = LocalCluster()
    client = Client(cluster)
    print(client.scheduler_info()['services'])

### read csv

    from dask.distributed import progress
    from distributed import Client
    import dask.dataframe as dd
    client = Client()

    dask_data=dd.read_csv('/local/home/rvshnkr/kaggle_data/NYT/train.csv')
    display(dask_data.head(2))

### data aggregation

    def agg(variable):
        agg1=dask_data.groupby([variable]).agg({'fare_amount':'mean','key':'size'}).compute().reset_index()
        agg1['proportion']=100*round(agg1['key']/agg1['key'].sum(),4)
        agg1['fare_amount']=round(agg1['fare_amount'],2)

        display(agg1[[variable,'fare_amount','proportion']][0:8])

    agg('passenger_count')
    agg('year')

### machine learning

    columns=['passenger_count','year','fare_amount']
    train_model=dask_data[columns]
    train_model=train_model.sample(frac=.05, replace=False)
    train_model.head()

    import dask_xgboost as dxgb
    params = { 'nround': 1000, 
            'max_depth': 16, 'eta': 0.01, 'subsample': 0.5, 
            'min_child_weight': 1,'scale_pos_weight':50}
    labels_train, labels_test = train_model['fare_amount'].random_split([0.9, 0.1], 
                                                        random_state=1234)
    data_train, data_test = train_model.random_split([0.9, 0.1], 
                                            random_state=1234)
    bst = dxgb.train(client, params, data_train, labels_train)
    predictions = dxgb.predict(client, bst, data_test).persist()
    labels=labels_test.compute()
    predictions=predictions.compute()

    from math import sqrt
    from sklearn.metrics import mean_squared_error
    Model_rmse = sqrt(mean_squared_error(labels_test, predictions))