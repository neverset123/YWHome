---
layout:     post
title:      NLP
subtitle:   
date:       2020-09-28
author:     neverset
header-img: img/post-bg-kuaidi.jpg
catalog: true
tags:
    - NLP
---

## StanfordNLP
CoreNLP is a library supporting all NLP operations like stemming, lementing, tokenization, finding parts of speech, sentiment analysis. StanfordNLP is a python wrapper for CoreNLP.

### Installation

    pip install stanfordnlp

### Usage example

    import stanfordnlp as st
    st.download(‘en’) 
    pipe = stanfordnlp.Pipeline()
    text = pipe("test text")

## Owl
Owl is word similarity API, it uses the largest word2vec English model created by spaCy (en-core-web-lg) for the general context and uses one of the word2vec models created at Stanford University (glove-wiki-gigaword-300) for the news context.  
the results are well-separated to the models, makers, and general subgroups

### Usage

    import requests
    url = "https://word-similarity.p.rapidapi.com/news/10/apple"
    headers = {
        'x-rapidapi-host': "word-similarity.p.rapidapi.com",
        'x-rapidapi-key': *** YOUR API KEY ***
        }
    response = requests.request("GET", url, headers=headers)
    print(response.text)

## cleaning text data

    # # In case of import errors
    # ! pip install nltk
    # ! pip install textblob
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    import re
    import nltk
    import string
    from nltk.corpus import stopwords

    # download all-nltk
    nltk.download()
    df = pd.read_csv('train.csv')
    stop_words = stopwords.words("english")
    wordnet = WordNetLemmatizer()
    def text_preproc(x):
        #lowercase the text
        x = x.lower()
        #remove stop words
        x = ' '.join([word for word in x.split(' ') if word not in stop_words])
        #remove unicode characters
        x = x.encode('ascii', 'ignore').decode()
        #remove url
        x = re.sub(r'https*\S+', ' ', x)
        #remove mentions
        x = re.sub(r'@\S+', ' ', x)
        #Remove Hashtags
        x = re.sub(r'#\S+', ' ', x)
        #Remove ticks and the next character
        x = re.sub(r'\'\w+', '', x)
        #Remove punctuations
        x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)
        #Remove numbers
        x = re.sub(r'\w*\d+\w*', '', x)
        #Replace the over spaces
        x = re.sub(r'\s{2,}', ' ', x)
        return x
    df['clean_text'] = df.text.apply(text_preproc)