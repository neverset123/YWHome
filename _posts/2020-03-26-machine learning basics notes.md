---
layout:     post
title:      machine learning basic notes
subtitle:   from coursa/xuantianlin
date:       2020-03-26
author:     neverset
header-img: img/post-bg-kuaidi.jpg
catalog: true
tags:
    - machine learning
---

## Linear regression
during linear regression, both Ein and Eout converge to sigma square, so the expected error( square error) between them is show in the pic.
d: vc dimension
N: Number of data points
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200326231657.png)

## logistic regression

### Error definition
the output of logistic regression is generated by sigmoid function
the logistic regression error is defined by maximizing the likelihood the h equal f, which is called crosss entropy error. this error is related to w, xn and yn
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200326234337.png)
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200326233017.png)

### Gradient decent and learning rate

the optimal gradient descent direction is opposite direction of Delta-Ein
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200326235248.png)

purple niu is teh learning rate
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200326235441.png)

the training process is described in the chart
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200326235751.png)

## Linear model
### advantages and disadvantages

demenstration of advantages and disadvantages between threee linear models
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200327000104.png)

![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200327000143.png)

### optimization process

what means stochastic gradient descent??

![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200327000806.png)

SGD logistic regression can be seen as a softed PLA
stop criterien during training: 
*iteration times
niu (learning rate )is set as 0.1(experience value)
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200327001000.png)

### multiclass classification

two methods to deal with multiclass classification
* OVA(one versus all)[not recommended, not clearly seperatable]
* OVO(one versus one )

![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200327001634.png)
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200327001548.png)

## nonlinear transformation

the principle to deal with nonlinear problem is to transform the data from nonlinear space to linear space, and use linear model to deal with it. However this will lead to more complex model, so there are some parameters( C, lambda) to restrict the complexity of the model (regularization???)


![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200327002150.png)
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200327002314.png)

## overfitting

use more complex model is not good expecially when number of data points is small
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200327002855.png)

four reasons leading to oversfitting:
* data size
* stochastic noise
* deterministic noise
* excessive power (model complexity)
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200327003033.png)

suggested ways to avoid overfitting:
![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200327003304.png)

![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20200327003444.png)